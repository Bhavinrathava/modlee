{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "This tutorial will walk you through building an end-to-end text classification pipeline using the `Modlee` package and `PyTorch Lightning`. \n",
    "\n",
    "We'll use the `Amazon Polarity` dataset, which contains customer reviews labeled as positive or negative, to build a simple binary classification model.\n",
    "\n",
    "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/modlee/text-classification)\n",
    "\n",
    "First, we will import the the necessary libraries and set up the environment. \n",
    "```python\n",
    "import os\n",
    "import torch\n",
    "import modlee\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from utils import get_device\n",
    "from datasets import load_dataset\n",
    "\n",
    "```\n",
    "Now, we will set up the `modlee` API key and initialize the `modlee` package. You can access your `modlee` API key [from the dashboard](https://www.dashboard.modlee.ai/).\n",
    "\n",
    "Replace `replace-with-your-api-key` with your API key.\n",
    "```python\n",
    "modlee.init(api_key=\"replace-with-your-api-key\")\n",
    "```\n",
    "\n",
    "Tokenization transforms raw text into input IDs and attention masks. We define a helper function `tokenize_texts` to handle this process. \n",
    "\n",
    "```python\n",
    "\n",
    "# Define a function to tokenize text data\n",
    "def tokenize_texts(texts, tokenizer, max_length=20):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,  # Truncate sequences longer than max_length\n",
    "        padding=\"max_length\",  # Pad shorter sequences to max_length\n",
    "        max_length=max_length,  # Maximum sequence length\n",
    "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "        add_special_tokens=True,  # Add special tokens like [CLS] and [SEP]\n",
    "    )\n",
    "    # Extract token IDs and attention masks as tensors\n",
    "    input_ids = encodings['input_ids'].to(torch.long)\n",
    "    attention_mask = encodings['attention_mask'].to(torch.long)\n",
    "    return input_ids, attention_mask\n",
    "```\n",
    "\n",
    "The `load_real_data` function loads the Amazon Polarity dataset, which contains customer reviews and their corresponding labels (positive or negative). We extract the text data and labels, limiting the dataset to 100 samples for simplicity in this example.\n",
    "\n",
    "```python\n",
    "def load_real_data(dataset_name=\"amazon_polarity\"):\n",
    "    dataset = load_dataset(\"amazon_polarity\", split='train[:80%]')\n",
    "    texts = dataset['content'] # Extract text data\n",
    "    targets = dataset['label'] # Extract labels\n",
    "    return texts, targets\n",
    "    \n",
    "# Load and preprocess the dataset\n",
    "texts, targets = load_real_data(dataset_name=\"amazon_polarity\")\n",
    "texts, targets = texts[:100], targets[:100]  # Use only the first 100 samples for simplicity\n",
    "```\n",
    "\n",
    "To evaluate the model, we split the data into training and testing subsets. The `train_test_split` function ensures that 80% of the data is used for training and 20% for testing. \n",
    "\n",
    "```python\n",
    "# Tokenize the text data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "input_ids, attention_masks = tokenize_texts(texts, tokenizer)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train, y_test = train_test_split(\n",
    "    input_ids, attention_masks, targets, test_size=0.2, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "DataLoaders enable efficient processing by dividing the dataset into smaller batches for training. Here, we create separate DataLoaders for the training and testing datasets.\n",
    "\n",
    "```python\n",
    "# Create DataLoader objects for training and testing\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train_ids, dtype=torch.long),\n",
    "    torch.tensor(X_train_masks, dtype=torch.long),\n",
    "    torch.tensor(y_train, dtype=torch.long)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(X_test_ids, dtype=torch.long),\n",
    "    torch.tensor(X_test_masks, dtype=torch.long),\n",
    "    torch.tensor(y_test, dtype=torch.long)\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "```\n",
    "\n",
    "We create a custom text classification model by inheriting from Modlee’s `TextClassificationModleeModel`. \n",
    "\n",
    "```python\n",
    "class ModleeTextClassificationModel(modlee.model.TextClassificationModleeModel):\n",
    "    def __init__(self, vocab_size, embed_dim=50, num_classes=2, tokenizer=None):\n",
    "        super().__init__()\n",
    "        # Embedding layer to map words to dense vectors\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim, padding_idx=tokenizer.pad_token_id if tokenizer else None)\n",
    "        # Sequential model containing a Flatten layer and fully connected layers\n",
    "        self.model = torch.nn.Sequential(\n",
    "            self.embedding,\n",
    "            torch.nn.Flatten(),  # Flatten embeddings for linear layers\n",
    "            torch.nn.Linear(embed_dim * 20, 128),  # Fully connected layer\n",
    "            torch.nn.ReLU(),  # ReLU activation\n",
    "            torch.nn.Linear(128, num_classes)  # Output layer for classification\n",
    "        )\n",
    "        # Define the loss function (cross-entropy for classification)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        if isinstance(input_ids, list):\n",
    "            input_ids = torch.cat(input_ids, dim=0)\n",
    "        embedded = self.embedding(input_ids)  # Pass input through the embedding layer\n",
    "        for layer in list(self.model.children())[1:]:  # Apply the rest of the layers\n",
    "            embedded = layer(embedded)\n",
    "        return embedded\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        preds = self.forward(input_ids, attention_mask)  # Get predictions\n",
    "        loss = self.loss_fn(preds, labels)  # Compute loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        preds = self.forward(input_ids, attention_mask)\n",
    "        loss = self.loss_fn(preds, labels)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "```\n",
    "\n",
    "We instantiate the model and use `PyTorch Lightning’s Trainer` class to handle training. \n",
    "\n",
    "```python\n",
    "# Initialize the model \n",
    "modlee_model = ModleeTextClassificationModel(\n",
    "    vocab_size=tokenizer.vocab_size, num_classes=2, tokenizer=tokenizer\n",
    ").to(device)\n",
    "\n",
    "# Train the model using Modlee and PyTorch Lightning's Trainer\n",
    "with modlee.start_run() as run:\n",
    "    trainer = pl.Trainer(max_epochs=1) # Train for one epoch\n",
    "    trainer.fit(\n",
    "        model=modlee_model,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=test_dataloader\n",
    "    )\n",
    "```\n",
    "\n",
    "\n",
    "After training, we inspect the artifacts saved by Modlee, including the model graph and various statistics. With Modlee, your training assets are automatically saved, preserving valuable insights for future reference and collaboration.\n",
    "\n",
    "```python\n",
    "last_run_path = modlee.last_run_path()\n",
    "print(f\"Run path: {last_run_path}\")\n",
    "artifacts_path = os.path.join(last_run_path, 'artifacts')\n",
    "artifacts = sorted(os.listdir(artifacts_path))\n",
    "print(f\"Saved artifacts: {artifacts}\")\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
