{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Regression\n",
    "\n",
    "This tutorial will walk you through building an end-to-end text regression pipeline using the `Modlee` package and `PyTorch Lightning`.\n",
    "\n",
    "We'll use the `Yelp Polarity` dataset, which contains customer reviews labeled with sentiment scores, to build a simple regression model that predicts a continuous value based on the text. \n",
    "\n",
    "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/modlee/text-regression)\n",
    "\n",
    "First, we will import the the necessary libraries and set up the environment. \n",
    "```python\n",
    "import os\n",
    "import torch\n",
    "import modlee\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from utils import get_device\n",
    "from datasets import load_dataset\n",
    "\n",
    "```\n",
    "Now, we will set up the `modlee` API key and initialize the `modlee` package. You can access your `modlee` API key [from the dashboard](https://www.dashboard.modlee.ai/).\n",
    "\n",
    "Replace `replace-with-your-api-key` with your API key.\n",
    "```python\n",
    "modlee.init(api_key=\"replace-with-your-api-key\")\n",
    "```\n",
    "\n",
    "Text data needs to be tokenized (converted into numerical format) before it can be used by machine learning models. We use a pre-trained BERT tokenizer for this.\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "```\n",
    "\n",
    "We define a function to preprocess raw text data using the tokenizer. Tokenization ensures that the input data has a uniform format and length, making it suitable for training deep learning models.\n",
    "\n",
    "```python\n",
    "def tokenize_texts(texts, tokenizer, max_length=20):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,  # Shorten longer texts\n",
    "        padding=\"max_length\",  # Pad shorter texts to a fixed length\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",  # Return data as PyTorch tensors\n",
    "        add_special_tokens=True,  # Include tokens like [CLS] and [SEP]\n",
    "    )\n",
    "    input_ids = encodings['input_ids'].to(torch.long)  # Token IDs\n",
    "    attention_mask = encodings['attention_mask'].to(torch.long)  # Padding masks\n",
    "    return input_ids, attention_mask\n",
    "```\n",
    "\n",
    "In this step, we load a text dataset using Hugging Face's `datasets` library. We are using the **Yelp Polarity** dataset, which consists of movie reviews labeled as positive or negative. \n",
    "\n",
    "```python\n",
    "def load_real_data(dataset_name):\n",
    "    # Load the dataset based on the provided name.\n",
    "    # In this case, we are specifically loading the 'yelp_polarity' dataset.\n",
    "    dataset = load_dataset(\"yelp_polarity\", split='train[:80%]')  # Load the first 80% of the training data\n",
    "\n",
    "    # Extract the 'text' column from the dataset, which contains the review texts.\n",
    "    texts = dataset['text']\n",
    "    \n",
    "    # Extract the 'label' column, which contains the sentiment labels (positive or negative).\n",
    "    targets = dataset['label']\n",
    "    \n",
    "    # Return the texts and their corresponding sentiment labels (targets).\n",
    "    return texts, targets\n",
    "```\n",
    "\n",
    "We tokenize the dataset and split it into training and testing sets. This step ensures that we have separate datasets for training and evaluation.\n",
    "\n",
    "```python\n",
    "# Load 'yelp_polarity' dataset\n",
    "texts, targets = load_real_data(dataset_name=\"yelp_polarity\")\n",
    "\n",
    "# Use only the first 100 samples for simplicity\n",
    "texts = texts[:100]  \n",
    "targets = targets[:100]\n",
    "\n",
    "# Tokenize the text into input IDs and attention masks\n",
    "input_ids, attention_masks = tokenize_texts(texts, tokenizer)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train_ids, X_test_ids, X_train_masks, X_test_masks, y_train, y_test = train_test_split(\n",
    "    input_ids, attention_masks, targets, test_size=0.2, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "We prepare PyTorch `DataLoader` objects to feed data into the model during training. \n",
    "\n",
    "```python\n",
    "# Create TensorDataset for training data\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train_ids, dtype=torch.long),\n",
    "    torch.tensor(X_train_masks, dtype=torch.long),\n",
    "    torch.tensor(y_train, dtype=torch.float)\n",
    ")\n",
    "\n",
    "# Create TensorDataset for testing data\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(X_test_ids, dtype=torch.long),\n",
    "    torch.tensor(X_test_masks, dtype=torch.long),\n",
    "    torch.tensor(y_test, dtype=torch.float)\n",
    ")\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create DataLoader for testing data\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Add tokenizer to the training dataloader\n",
    "train_dataloader.initial_tokenizer = tokenizer\n",
    "```\n",
    "\n",
    "We create a custom text regression model by inheriting from Modlee’s `TextRegressionModleeModel`. \n",
    "\n",
    "```python\n",
    "class ModleeTextRegressionModel(modlee.model.TextRegressionModleeModel):\n",
    "    def __init__(self, vocab_size, embed_dim=50, tokenizer=None):\n",
    "        # Initialize the parent class to inherit Modlee functionality\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create an embedding layer to convert token IDs into dense vectors\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim, padding_idx=tokenizer.pad_token_id if tokenizer else None)\n",
    "        \n",
    "        # Define the rest of the model architecture\n",
    "        self.model = torch.nn.Sequential(\n",
    "            self.embedding,  # Convert token IDs into embeddings\n",
    "            torch.nn.Flatten(),  # Flatten the embedded vectors for linear layers\n",
    "            torch.nn.Linear(embed_dim * 20, 128),  # Linear layer with 128 hidden units\n",
    "            torch.nn.ReLU(),  # ReLU activation function for non-linearity\n",
    "            torch.nn.Linear(128, 1)  # Output layer that produces a single regression value\n",
    "        )\n",
    "        \n",
    "        # Define the loss function for regression (Mean Squared Error Loss)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # The forward pass takes input_ids (tokenized text) and attention_mask (if applicable)\n",
    "        \n",
    "        # If input_ids are provided as a list, concatenate them to form a single tensor\n",
    "        if isinstance(input_ids, list):\n",
    "            input_ids = torch.cat(input_ids, dim=0)\n",
    "        \n",
    "        # Pass the input_ids through the embedding layer\n",
    "        embedded = self.embedding(input_ids)\n",
    "        \n",
    "        # Process the embedded vectors through the model's layers\n",
    "        for layer in list(self.model.children())[1:]:  # Skip embedding layer (already applied)\n",
    "            embedded = layer(embedded)  # Pass through each layer (Flatten, Linear, ReLU, etc.)\n",
    "        \n",
    "        return embedded  # Return the final prediction (single value)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # This function is used during the training loop for a single batch\n",
    "        input_ids, attention_mask, targets = batch  # Unpack the batch\n",
    "        \n",
    "        # Get the model predictions for the current batch\n",
    "        preds = self.forward(input_ids, attention_mask)\n",
    "        \n",
    "        # Calculate the loss between the predictions and the true targets\n",
    "        loss = self.loss_fn(preds.squeeze(), targets)  # Squeeze to remove any extra dimensions\n",
    "        return loss  # Return the loss to be used by the optimizer\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # This function is used during validation to calculate loss\n",
    "        input_ids, attention_mask, targets = batch  # Unpack the batch\n",
    "        \n",
    "        # Get the model predictions for the current batch\n",
    "        preds = self.forward(input_ids, attention_mask)\n",
    "        \n",
    "        # Calculate the validation loss between predictions and targets\n",
    "        loss = self.loss_fn(preds.squeeze(), targets)  # Squeeze to remove any extra dimensions\n",
    "        return loss  # Return the validation loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Configure the optimizer (Adam optimizer with learning rate of 1e-3)\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "```\n",
    "\n",
    "We instantiate the model and use `PyTorch Lightning’s Trainer` class to handle training.\n",
    "\n",
    "```python\n",
    "# Initialize the model \n",
    "modlee_model = ModleeTextRegressionModel(\n",
    "    vocab_size=tokenizer.vocab_size, tokenizer=tokenizer\n",
    ").to(device)\n",
    "\n",
    "# Train the model using Modlee and PyTorch Lightning's Trainer\n",
    "with modlee.start_run() as run:\n",
    "    trainer = pl.Trainer(max_epochs=1) # Train for one epoch\n",
    "    trainer.fit(\n",
    "        model=modlee_model,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=test_dataloader\n",
    "    )\n",
    "```\n",
    "\n",
    "After training, we inspect the artifacts saved by Modlee, including the model graph and various statistics. With Modlee, your training assets are automatically saved, preserving valuable insights for future reference and collaboration.\n",
    "\n",
    "```python\n",
    "last_run_path = modlee.last_run_path()\n",
    "print(f\"Run path: {last_run_path}\")\n",
    "artifacts_path = os.path.join(last_run_path, 'artifacts')\n",
    "artifacts = sorted(os.listdir(artifacts_path))\n",
    "print(f\"Saved artifacts: {artifacts}\")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
