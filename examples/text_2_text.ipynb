{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text2Text Example\n",
    "\n",
    "This tutorial provides a detailed walkthrough of building an end-to-end sequence-to-sequence pipeline using the `Modlee` package and `PyTorch Lightning`.\n",
    "\n",
    "We'll use the Romanian-English translation subset from the `WMT16` dataset to create a transformer-based model capable of translating English sentences into Romanian. \n",
    "\n",
    "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/modlee/text2text)\n",
    "\n",
    "First, we will import the the necessary libraries and set up the environment. \n",
    "```python\n",
    "import os\n",
    "import modlee\n",
    "import lightning.pytorch as pl\n",
    "from utils import check_artifacts, get_device\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "```\n",
    "Now, we will set up the `modlee` API key and initialize the `modlee` package. You can access your `modlee` API key [from the dashboard](https://www.dashboard.modlee.ai/).\n",
    "\n",
    "Replace `replace-with-your-api-key` with your API key.\n",
    "```python\n",
    "modlee.init(api_key=\"replace-with-your-api-key\")\n",
    "```\n",
    "\n",
    "These constants determine the batch size (number of samples per batch) and the maximum sequence length for tokenized inputs.\n",
    "\n",
    "```python\n",
    "# Number of samples processed in each batch\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Maximum number of tokens per sequence\n",
    "MAX_LENGTH = 50\n",
    "```\n",
    "\n",
    "Now, we will load and tokenize the dataset. We use `datasets.load_dataset()` to fetch a translation dataset. The `AutoTokenizer` tokenizes both English and Romanian texts, truncating or padding them to max_length.\n",
    "\n",
    "```python\n",
    "def load_dataset_and_tokenize(num_samples, dataset_name=\"wmt16\", subset=\"ro-en\", split=\"train[:80%]\", max_length=50):\n",
    "    \"\"\"\n",
    "    Load a dataset and tokenize it for text-to-text tasks.\n",
    "    Args:\n",
    "        num_samples (int): Number of samples to load.\n",
    "        dataset_name (str): Dataset to use (default: \"wmt16\").\n",
    "        subset (str): Specific subset of the dataset (e.g., Romanian-English).\n",
    "        split (str): Portion of data to use (default: \"train[:80%]\").\n",
    "        max_length (int): Maximum sequence length for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Encoded input IDs, target IDs, and tokenizer instance.\n",
    "    \"\"\"\n",
    "    # Load the specified dataset from Hugging Face's Datasets library\n",
    "    dataset = load_dataset(dataset_name, subset, split=split)\n",
    "\n",
    "    # Select only the first `num_samples` samples for quick testing\n",
    "    subset = dataset.select(range(num_samples))\n",
    "    texts = [item['translation']['en'] for item in subset]  # Source texts (English)\n",
    "    target_texts = [item['translation']['ro'] for item in subset]  # Target texts (Romanian)\n",
    "\n",
    "    # Load a pre-trained tokenizer (e.g., T5 tokenizer)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "    # Tokenize the source texts\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,  # Truncate sequences to the specified max_length\n",
    "        padding=\"max_length\",  # Pad sequences to the max_length\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",  # Return tokenized data as PyTorch tensors\n",
    "        add_special_tokens=True,  # Add necessary special tokens like <pad> or <eos>\n",
    "    )\n",
    "\n",
    "    # Tokenize the target texts similarly\n",
    "    target_encodings = tokenizer(\n",
    "        target_texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    # Convert encodings to PyTorch tensors with the appropriate data type\n",
    "    input_ids = encodings['input_ids'].to(torch.long)\n",
    "    decoder_input_ids = target_encodings['input_ids'].to(torch.long)\n",
    "\n",
    "    return input_ids, decoder_input_ids, tokenizer\n",
    "```\n",
    "\n",
    "This function splits the dataset into training and validation sets. `train_test_split` ensures the data is randomly divided, with 80% for training and 20% for validation.\n",
    "\n",
    "```python\n",
    "def create_dataloaders(input_ids, decoder_input_ids, test_size=0.2, batch_size=16):\n",
    "    \"\"\"\n",
    "    Split data into training and validation sets and create PyTorch DataLoaders.\n",
    "    Args:\n",
    "        input_ids (Tensor): Input token IDs.\n",
    "        decoder_input_ids (Tensor): Decoder token IDs.\n",
    "        test_size (float): Proportion of data for validation (default: 20%).\n",
    "        batch_size (int): Number of samples per batch (default: 16).\n",
    "\n",
    "    Returns:\n",
    "        tuple: DataLoaders for training and validation.\n",
    "    \"\"\"\n",
    "    # Split the input and target data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        input_ids, decoder_input_ids, test_size=test_size, random_state=42\n",
    "    )\n",
    "\n",
    "    # Wrap the training and validation data into PyTorch Datasets\n",
    "    train_dataset = TensorDataset(X_train, X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, X_val, y_val)\n",
    "\n",
    "    # Create DataLoaders to efficiently load batches of data\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "```\n",
    "\n",
    "Next, we initialize a transformer-based model:\n",
    "\n",
    "- Embedding Layer: Converts token IDs into dense vectors of size `d_model`.\n",
    "- Positional Encoding: Adds positional context to embeddings, as transformers are position-agnostic.\n",
    "- Transformer Module: Implements the encoder-decoder architecture.\n",
    "- Output Layer: A fully connected layer projects transformer outputs to the vocabulary space.\n",
    "  \n",
    "The `_generate_positional_encoding` function creates sinusoidal encodings, which are added to token embeddings to capture sequence order.\n",
    "\n",
    "```python\n",
    "class TransformerSeq2SeqModel(modlee.model.TextTextToTextModleeModel):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, max_length=50):\n",
    "        # Initialize a Transformer-based sequence-to-sequence model.\n",
    "        super(TransformerSeq2SeqModel, self).__init__()\n",
    "\n",
    "        # Embedding layer converts token indices into dense vectors of size `d_model`.\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Pre-compute positional encodings for input and target sequences to add positional context.\n",
    "        self.positional_encoding = nn.Parameter(\n",
    "            self._generate_positional_encoding(d_model, max_length), requires_grad=False\n",
    "        )\n",
    "\n",
    "        # Transformer module with customizable encoder and decoder configurations.\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,              # Model dimensionality\n",
    "            nhead=nhead,                  # Number of attention heads in multi-head attention\n",
    "            num_encoder_layers=num_encoder_layers,  # Number of encoder layers\n",
    "            num_decoder_layers=num_decoder_layers,  # Number of decoder layers\n",
    "            dim_feedforward=2048,         # Size of the feedforward network\n",
    "            dropout=0.1                   # Dropout rate for regularization\n",
    "        )\n",
    "\n",
    "        # Fully connected output layer maps Transformer outputs back to the vocabulary space.\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # Store model parameters for reference.\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, input_ids, decoder_input_ids=None):\n",
    "        # If no decoder input is provided, use the encoder input (e.g., for auto-regressive tasks).\n",
    "        if decoder_input_ids is None:\n",
    "            decoder_input_ids = input_ids\n",
    "\n",
    "        # Handle case where inputs are provided as a tuple of encoder and decoder inputs.\n",
    "        if isinstance(input_ids, list) and len(input_ids) == 2:\n",
    "            (input_ids, decoder_input_ids) = input_ids\n",
    "\n",
    "        # Add positional encoding to the embeddings for both source and target sequences.\n",
    "        src = self.embedding(input_ids) * (self.d_model ** 0.5) + self.positional_encoding[:input_ids.size(1), :]\n",
    "        tgt = self.embedding(decoder_input_ids) * (self.d_model ** 0.5) + self.positional_encoding[:decoder_input_ids.size(1), :]\n",
    "\n",
    "        # Adjust dimensions to fit the Transformer module's expected input format (seq_len, batch, d_model).\n",
    "        src = src.permute(1, 0, 2)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "\n",
    "        # Encode the source sequence to produce a memory representation.\n",
    "        memory = self.transformer.encoder(src)\n",
    "\n",
    "        # Decode the target sequence using the encoder's memory.\n",
    "        output = self.transformer.decoder(tgt, memory)\n",
    "\n",
    "        # Project the Transformer output to the vocabulary space and adjust dimensions back to (batch, seq_len, vocab_size).\n",
    "        logits = self.fc_out(output.permute(1, 0, 2))\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Unpack the batch data into encoder inputs, decoder inputs, and labels.\n",
    "        input_ids, decoder_input_ids, labels = batch\n",
    "\n",
    "        # Forward pass through the model to generate logits.\n",
    "        logits = self(input_ids, decoder_input_ids)\n",
    "\n",
    "        # Compute cross-entropy loss between predictions and true labels.\n",
    "        loss = nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Similar to the training step but used for validation.\n",
    "        input_ids, decoder_input_ids, labels = batch\n",
    "        logits = self(input_ids, decoder_input_ids)\n",
    "        loss = nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Use the Adam optimizer with a learning rate of 5e-5 for training.\n",
    "        return torch.optim.Adam(self.parameters(), lr=5e-5)\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_positional_encoding(d_model, max_length):\n",
    "        # Generate sinusoidal positional encodings based on sequence position and model dimensionality.\n",
    "        pos = torch.arange(0, max_length).unsqueeze(1)\n",
    "        i = torch.arange(0, d_model, 2)\n",
    "        angle_rates = 1 / torch.pow(10000, (i.float() / d_model))\n",
    "\n",
    "        # Initialize positional encodings and calculate sine and cosine functions for even and odd indices.\n",
    "        pos_enc = torch.zeros(max_length, d_model)\n",
    "        pos_enc[:, 0::2] = torch.sin(pos * angle_rates)\n",
    "        pos_enc[:, 1::2] = torch.cos(pos * angle_rates)\n",
    "\n",
    "        return pos_enc\n",
    "```\n",
    "\n",
    "We instantiate the model and use `PyTorch Lightning’s Trainer` class to handle training. The Trainer manages training loops, validation, and logging.\n",
    "\n",
    "```python\n",
    "# Load data and tokenize it\n",
    "input_ids, decoder_input_ids, tokenizer = load_dataset_and_tokenize(num_samples=100)\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "train_dataloader, val_dataloader = create_dataloaders(input_ids, decoder_input_ids)\n",
    "\n",
    "# Initialize the transformer model with the tokenizer's vocabulary size\n",
    "model = TransformerSeq2SeqModel(vocab_size=tokenizer.vocab_size)\n",
    "\n",
    "# Use PyTorch Lightning's Trainer to handle training and validation\n",
    "with modlee.start_run() as run:\n",
    "    trainer = pl.Trainer(max_epochs=1) # Train for one epoch\n",
    "    trainer.fit(\n",
    "        model=modlee_model,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=test_dataloader\n",
    "    )\n",
    "```\n",
    "\n",
    "After training, we inspect the artifacts saved by Modlee, including the model graph and various statistics. With Modlee, your training assets are automatically saved, preserving valuable insights for future reference and collaboration.\n",
    "\n",
    "```python\n",
    "last_run_path = modlee.last_run_path()\n",
    "print(f\"Run path: {last_run_path}\")\n",
    "artifacts_path = os.path.join(last_run_path, 'artifacts')\n",
    "artifacts = sorted(os.listdir(artifacts_path))\n",
    "print(f\"Saved artifacts: {artifacts}\")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
